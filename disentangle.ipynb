{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "95KPloxzR5n-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "torch.set_printoptions(precision=10)\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "r4gqLMcDTWae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64]) torch.Size([1, 3, 64, 64]) torch.Size([1, 1, 64, 64])\n",
      "torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "#load image\n",
    "true_image = cv2.imread(\"coloured.jpg\")\n",
    "true_mask = cv2.imread(\"mask.jpg\")\n",
    "#m3 = cv2.merge((m,m,m))\n",
    "\n",
    "#preprocess image\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()\n",
    "     ])\n",
    "\n",
    "def inverse_normalization(x):\n",
    "    x *= 255 \n",
    "    x = torch.clamp(x, 0, 255)\n",
    "    #x = x.numpy()\n",
    "    return x\n",
    "\n",
    "# print(true_image.shape, true_mask.shape)\n",
    "# true_image_tensor = torch.from_numpy(true_image).float()\n",
    "# true_image_tensor = true_image_tensor.permute(2,0,1)\n",
    "# true_mask_tensor = torch.from_numpy(true_mask).float()\n",
    "# true_mask_tensor = true_mask_tensor.permute(2,0,1)\n",
    "\n",
    "input_image = transform(true_image).unsqueeze(0)\n",
    "gt_image = input_image.clone()\n",
    "gt_mask = transform(true_mask).unsqueeze(0)\n",
    "gt_mask = gt_mask.mean(axis=1).unsqueeze(0)\n",
    "print(input_image.shape, gt_image.shape, gt_mask.shape)\n",
    "print(input_image.dtype, gt_image.dtype, gt_mask.dtype)\n",
    "\n",
    "#Normalize 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOnW20xdUCwJ",
    "outputId": "12c24c1b-5c07-4031-dd3f-bb16ad9044d6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.9098039269))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image.min(), input_image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.9098039269))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_image.min(), gt_image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.), torch.Size([1, 1, 64, 64]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_mask.min(), gt_mask.max(), gt_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_mask_tensor.min(), true_mask_tensor.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "JeBy36OJTcP9"
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "        def __init__(self):\n",
    "                super(encoder, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(3,64, kernel_size=3)\n",
    "                self.conv2 = nn.Conv2d(64,32, kernel_size=3)\n",
    "                self.conv3 = nn.Conv2d(32,16, kernel_size=3)\n",
    "                \n",
    "                self.conv1d1 = nn.Conv1d(16*58*58, 1024, kernel_size=1)\n",
    "                self.conv1d2 = nn.Conv1d(1024,512, kernel_size=1)\n",
    "                self.conv1d3 = nn.Conv1d(512,256, kernel_size=1)\n",
    "\n",
    "                self.upconv1 = nn.ConvTranspose2d(16, 32, kernel_size=3, stride=1)\n",
    "                self.upconv2 = nn.ConvTranspose2d(32, 64, kernel_size=3, stride=1)\n",
    "                self.upconv3 = nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1)\n",
    "                \n",
    "   \n",
    "        def forward(self,x):\n",
    "                #Encodings\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x = F.relu(self.conv3(x))\n",
    "                #print(\"X shape\", x.shape) # torch.Size([1, 16, 58, 58])\n",
    "                \n",
    "                #Style vector\n",
    "                x_style = torch.flatten(x,1)\n",
    "                x_style = x_style.unsqueeze(-1)\n",
    "                x_style = F.relu(self.conv1d1(x_style))\n",
    "                x_style = F.relu(self.conv1d2(x_style))\n",
    "                x_style = F.relu(self.conv1d3(x_style))\n",
    "                #print(\"Final X_Style: \", x_style.shape)\n",
    "\n",
    "                #Image \n",
    "                x_image = F.relu(self.upconv1(x))\n",
    "                x_image = F.relu(self.upconv2(x_image))\n",
    "                x_image = self.upconv3(x_image)\n",
    "            \n",
    "#                 x_style = inverse_normalization(x_style)\n",
    "#                 x_image = inverse_normalization(x_image)\n",
    "                \n",
    "#                 x_image = inverse_normalization(x_image)\n",
    "                \n",
    "                return x_style, x_image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "id": "5c9YiNG57qEQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class decoder(nn.Module):\n",
    "        def __init__(self):\n",
    "              super(decoder, self).__init__()\n",
    "                  \n",
    "              #Convert style for concat\n",
    "              self.conv1d0 = nn.Conv1d(256,512, kernel_size=1)\n",
    "\n",
    "              #Convert image for concat\n",
    "              self.conv1 = nn.Conv2d(1,64, kernel_size=3)\n",
    "              self.conv2 = nn.Conv2d(64,32, kernel_size=3)\n",
    "              self.conv3 = nn.Conv2d(32,16, kernel_size=3)\n",
    "              self.conv1d1 = nn.Conv1d(16*58*58, 1024, kernel_size=1)\n",
    "              self.conv1d2 = nn.Conv1d(1024,512, kernel_size=1)\n",
    "\n",
    "              #Convert image\n",
    "              self.conv1d3 = nn.Conv1d(1024, 1024, kernel_size=1)\n",
    "              self.conv1d4 = nn.Conv1d(1024, 1024, kernel_size=1)\n",
    "\n",
    "              #Using Upsample + Upconv\n",
    "   \n",
    "              self.upsample1 = nn.Upsample(scale_factor=2,mode='nearest')\n",
    "              self.upconv1 = nn.ConvTranspose2d(16, 16, 1, stride=1)\n",
    "              self.upsample2 = nn.Upsample(scale_factor=2,mode='nearest')\n",
    "              self.upconv2 = nn.ConvTranspose2d(16, 16, 1, stride=1)              \n",
    "              self.upsample3 = nn.Upsample(scale_factor=2,mode='nearest')\n",
    "              self.upconv3 = nn.ConvTranspose2d(16, 3, 1, stride=1)\n",
    "       \n",
    "              \n",
    "                \n",
    "\n",
    "        def forward(self,x_style,x_image):\n",
    "              \n",
    "#               x_image /= 255.0 \n",
    "              #x_style = torch.from_numpy(x_style)\n",
    "              #x_image = torch.from_numpy(x_image)\n",
    "                \n",
    "              x1 = F.relu(self.conv1d0(x_style)) # 512 style\n",
    "            \n",
    "              x2 = F.relu(self.conv1(x_image))\n",
    "              x2 = F.relu(self.conv2(x2))\n",
    "              x2 = F.relu(self.conv3(x2))\n",
    "              x2 = torch.flatten(x2, 1)\n",
    "              x2 = x2.unsqueeze(-1)\n",
    "              x2 = F.relu(self.conv1d1(x2))\n",
    "              x2 = F.relu(self.conv1d2(x2))     # 512 image\n",
    "\n",
    "              x = torch.cat([x1,x2], dim=1) #Linear concatenate, output = [1,1024]\n",
    "\n",
    "              x = F.relu(self.conv1d3(x))\n",
    "              x = F.relu(self.conv1d4(x))\n",
    "              x = (torch.reshape(x, (16,8,8))).unsqueeze(dim=0)\n",
    "\n",
    "              x = self.upsample1(x)\n",
    "              x = F.relu(self.upconv1(x))\n",
    "              x = self.upsample2(x)\n",
    "              x = F.relu(self.upconv2(x))\n",
    "              x = self.upsample3(x)\n",
    "              x = self.upconv3(x)\n",
    "\n",
    "#               x = inverse_normalization(x)\n",
    "            \n",
    "              return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "36EonbuHBXJy"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "      super(MyModel, self).__init__()\n",
    "      self.enc = enc\n",
    "      self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "      style, output_mask = self.enc(x)\n",
    "      output_img = self.dec(style, output_mask)\n",
    "      return output_img, output_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "S2TeS2G_XwCJ"
   },
   "outputs": [],
   "source": [
    "enc = encoder()\n",
    "dec = decoder()\n",
    "model = MyModel(enc,dec)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for g in optimizer.param_groups:\n",
    "#    g['lr'] = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "Yjh8fybjWdfk",
    "outputId": "8bdcb7a2-9b12-45fc-85eb-7f8e46a0313f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 150 | loss_total = 0.090 , loss_mask = 0.004, loss_style = 0.086\n",
      "Epoch = 151 | loss_total = 0.089 , loss_mask = 0.004, loss_style = 0.086\n",
      "Epoch = 152 | loss_total = 0.089 , loss_mask = 0.004, loss_style = 0.085\n",
      "Epoch = 153 | loss_total = 0.088 , loss_mask = 0.004, loss_style = 0.085\n",
      "Epoch = 154 | loss_total = 0.088 , loss_mask = 0.004, loss_style = 0.084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(150,155):\n",
    "    #Training Encoder\n",
    "    optimizer.zero_grad() \n",
    "    out_image, out_mask = model(input_image)\n",
    "\n",
    "#     print(out_image.shape, out_mask.shape)\n",
    "#     print(gt_image.shape, gt_mask.shape)\n",
    "#     print(out_image.dtype, out_mask.dtype)\n",
    "#     print(gt_image.dtype, gt_mask.dtype)\n",
    "\n",
    "    loss_mask = criterion(out_mask.squeeze(0), gt_mask)\n",
    "    loss_style = criterion(out_image.squeeze(0), gt_image)\n",
    "    total_loss = loss_mask + loss_style\n",
    "\n",
    "#     print(total_loss.item())\n",
    "#     if epoch % 5 == 0:\n",
    "#     print(\"Epoch:\", epoch, \" Loss_mask: {:.3f} Loss style: {:.3f} \".format(loss_mask.item(), loss_style.item()))\n",
    "    print(f'Epoch = {epoch} | loss_total = {total_loss.item():.3f} , loss_mask = {loss_mask.item():.3f}, loss_style = {loss_style.item():.3f}')\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_image, out_mask = model(input_image)\n",
    "out_image, out_mask = out_image.detach(), out_mask.detach()\n",
    "out_image, out_mask = torch.clamp(out_image, 0, 1), torch.clamp(out_mask, 0, 1)\n",
    "# out_image = (out_image - out_image.min()) / (out_image.max() - out_image.min())\n",
    "# out_mask = (out_mask - out_mask.min()) / (out_mask.max() - out_mask.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.7761616707), tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_image.min(), out_image.max(), out_mask.min(), out_mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_image = out_image.squeeze() * 255\n",
    "#out_mask = out_mask.squeeze() * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 64, 64]), torch.float32)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_image.squeeze().shape, gt_image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 64, 64]), torch.float32)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_image.squeeze().shape, out_image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FD65B7ED790>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(gt_image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABqklEQVR4nO2aTStEURiA77hnTHcwyEy+h9BkNUOKkpSsbNUs/Cu/QFgpWbC0kMzYjLIQMYXJQsRo3MF8+Rh/4Nm/nXqf5fOezXPfujVnbmDTaXWIrfQSetP2h97LPKGfuIuj/0wPo79s3KKPnxTRt6C1CA2QRgOk0QBpzL2ziIPkSA/6cPgD/WWTn8WLU0IfS02jH/Mb6P3THHrrN6AB0miANBogjUk4GRz0Xy2j3xgdQT94/4C+FvlEH3yOoA/9vqJvC/HvCus3oAHSaIA0GiCN2e6YwYE7GUTflQyhT/kr6PNuDX1klJ9dcY/vi97KA+it34AGSKMB0miANCbY4uNgvM73NuZiCv1OfRe92+D/E+bfevl8tIA+Fsijt34DGiCNBkijAdKYVf8LB/u/7L1KFv3a3hn6ducb/VFpls9XwugXygH01m9AA6TRAGk0QBqTjfH7dbjajb7e9NAfD/H9vf/O7/WxEH//81Opoj+Id6K3fgMaII0GSKMB0pim5+Igd3eDfv2czx9W+f4+4Rygz1xH0RcC/L3Q3GMfeus3oAHSaIA0GiDNP26yWgZqU2PYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FD65B835AF0>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(out_image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAEqElEQVR4nH1XPY8cRRB9r7pnP+7Dh+yTjCUkIwuJjMBAhuAPIH4BkPCbyBE5GSE5EglIBICQwAkyAlkG7nbvdqa7HkHP7s7M9l5HMz1dr169rqru4XvI7i7I5e4SQDIEI0ZD2d2zC4AxxPKdFkPUbUqdXJ4SrG0dsrkhxsARRE6ts73NEucLhVkTCI9Ns4jden2TvCMQzs5zK6XWVivGEEYMSJ1f5k2G3wp/A01DGpvTi4jc5Zw2CM3lRx9cNnNL69W3Xz7POWsIYM3Fh58+ns+Cbm5/+/ybtSdHNKYcc+66zhNc3ZO3LzBj61FfWZspAtihaHX/nTkAnOO1r2frZA4FzT1COcsB93z/UXCDpcXF1Q0gjChobfMtm/YvgwMZyjApl6X51gIEEDiFGyYjd//unpfbB5f2C5WL7BJs5Lv/jOvds8P34mAwXfQ2aP99MNJ05RQAAHoSVYDq5BBAu3U+EXD0ffQ0BhAgQIJXzMcM9lk6DGEHULVHrpgfYVADILLvn6sAdbKD2er0IQAJVWPYF8cREbczPCJi3okwBRjvj4RclSH7dukEYPsqFR01kGs/hJyLg7tDEDSMdsJAE+sxALndn3omZq/NHjDQcQa5BhAHtsX8aC65AzzQYAjgkDucVRGB3A1cHQFwz8g2bDODoZttfhytxrKPTPFBDeD8ui2WfoRBAZYZHzy92LzsdusEMJ6cPb0XrPd0ADDMRVp8/ZP2JCUJAEGBIoHHTwaAGgNsgQUJyidv2amhlG054Yy5vVeLaxQCAJA0xQfWEMU/+/L3kwBgfNhUACTAxUBwSABbtcXCcu+wAAxaDOEwOQGKgPYJDrEkUkXErXtJpGgOG2sA9JlKAK6Bx1EIkiQwxpYNDkdx7+PeNs4DSZIaX5tZ8Y9dBBufM2+p7G3iob2nq+//mSf3AkACtNkiP3yTHvyOYtrS9M2vX/z431Uq7vo8iBeX73/2arRemOMAgIvXP/x8MP3y2cnHvbnuOJkICo2twgEAcNU0Rk3bxfhwlQALiyW8gsDFIpCcFNO0nGGkBR0eLYYQQjCSOgbQa26sn3eg9UUo1TWgkSTHGlV8qeRjBYCsGg6W9gRYB4ABd2HYLnoeyYPdfB3EevlrDHrgfeLXGRRz92EyTxiQxjoKEQpHyY9tY29PTH8W9ksJ4DhA0ZAA64ng5fqg/dHJI3ekOgN3d9coDzQNoa/VvpuMhlB+idS35loIu0q3Oi+p/FKBdiSVyz6wCkA01mfK0H64jdwdI1ZrB1gEFpF1DMC2t5yaBoZlZH+HtD0CBwBmMAJklYHZwsDS+mncna5DAIAMFoQQDxgwhLn1/SgPLyA7ALOzGdAQwZAaTjsSw3JmjUPuabVhSUbSaCQNBNic+HYnNyfLyclEssG1lF1KqzyPBEgLwaLNzlKbsgL10xv3s7Hr0i/NMo/MQWv4/LsXcyK36983zcIEqInzhu9erVLXAdzkR5d+k+GYtc/y6bC1EKSns4exS5Is/3k7I2mI9spFRAhEEITu5YvbBABNmM/itDW5tX+k1gGGYOdlu5pmzshl8AxB2fu/NcZghxfNUkcAacb+c7Cl/Q+Wvn9+FCXxmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x7FD63DA32A90>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(out_mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bETcuyRuWJ2z"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7e4f69faf146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Convert tensors to images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresult_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moriginal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_image' is not defined"
     ]
    }
   ],
   "source": [
    "#Convert tensors to images\n",
    "\n",
    "result_image = transforms.ToPILImage()(output_image.squeeze(0))\n",
    "result_mask = transforms.ToPILImage()(output_mask.squeeze(0))\n",
    "original_mask = transforms.ToPILImage()(true_mask.squeeze(0))\n",
    "original_image = transforms.ToPILImage()(true_image.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "disentangle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vocc",
   "language": "python",
   "name": "vocc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
