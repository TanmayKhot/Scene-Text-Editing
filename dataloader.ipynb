{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#SSIM https://github.com/Po-Hsun-Su/pytorch-ssim\n",
    "#import pytorch_ssim\n",
    "from piq import ssim as ssim\n",
    "#from piqa import SSIM \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "torch.set_printoptions(precision=10)\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()\n",
    "     ])\n",
    "\n",
    "def get_imagetensor(location, mask=0):\n",
    "    \n",
    "    image = cv2.imread(location)\n",
    "    if mask == 0:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    elif mask == 1:\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "        image_tensor = image_tensor.mean(axis=1).unsqueeze(0)\n",
    "        \n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "\n",
    "def display_losses(epoch, total_loss, loss_mask, loss_style, loss_12, loss_13):\n",
    "    print(\"Epoch = \", epoch)\n",
    "    print(\"Total Loss = {:.3f}\".format(total_loss.item()))\n",
    "    print(\"Mask Loss = {:.3f}\".format(loss_mask.item()))\n",
    "    print(\"Style Loss = {:.3f}\".format(loss_style.item()))\n",
    "    print(\"12 Loss = {:.3f}\".format(loss_12.item()))\n",
    "    print(\"13 Loss = {:.3f}\".format(loss_13.item()))\n",
    "    print(\"Same Style Loss = {:.3f}\".format(loss_samestyle.item()))\n",
    "    print(\"Same mask Loss = {:.3f}\".format(loss_samemask.item()))    \n",
    "    return\n",
    "\n",
    "def display_images(image,mask,recon,flag=0):\n",
    "    \n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    mask = torch.clamp(mask, 0, 1)\n",
    "    if mask.shape[1] == 1:\n",
    "        mask = mask.repeat(1,3,1,1)\n",
    "    recon = torch.clamp(recon, 0, 1)\n",
    "    \n",
    "    if flag == 1:\n",
    "        print(\"SSIM Score = {:.3f}\".format( ssim(mask, recon).item()))\n",
    "    else:\n",
    "        print(\"SSIM Score = {:.3f}\".format( ssim(image, recon).item()))\n",
    "    \n",
    "    image = transforms.ToPILImage()(image.squeeze())\n",
    "    mask = transforms.ToPILImage()(mask.squeeze())\n",
    "    recon = transforms.ToPILImage()(recon.squeeze())\n",
    "    \n",
    "    fig =plt.figure(figsize=(5,5))\n",
    "    fig.add_subplot(1,3,1);plt.imshow(image)\n",
    "    fig.add_subplot(1,3,2);plt.imshow(mask)\n",
    "    fig.add_subplot(1,3,3);plt.imshow(recon)\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "    \n",
    "\n",
    "def display_all(gt_image, out_mask, out_image,\n",
    "                gt_ss_image, ss_out_mask, ss_out_image,\n",
    "                gt_sm_image, sm_out_mask, sm_out_image,\n",
    "                out12_image, out13_image\n",
    "               ):\n",
    "    print(\"Base Image (Image 1)\")\n",
    "    display_images(gt_image, out_mask, out_image)\n",
    "    \n",
    "    print(\"\\nSame Style (Image 2)\")\n",
    "    display_images(gt_ss_image, ss_out_mask, ss_out_image)\n",
    "    \n",
    "    print(\"\\nSame Mask (Image 3)\")\n",
    "    display_images(gt_sm_image, sm_out_mask, sm_out_image)\n",
    "    \n",
    "    print(\"\\nFirst Image style + Second Image Mask --> Expected Image 2, (Same style different mask)\")\n",
    "    display_images(gt_image, gt_ss_image, out12_image, flag=1)\n",
    "    \n",
    "    print(\"\\nThird Image style + First Image Mask --> Expected Image 3 (Differnet style same mask)\")\n",
    "    display_images(gt_sm_image, gt_image, out13_image)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "        def __init__(self):\n",
    "                super(encoder, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(3,64, kernel_size=3)\n",
    "                self.conv2 = nn.Conv2d(64,32, kernel_size=3)\n",
    "                self.conv3 = nn.Conv2d(32,16, kernel_size=3)\n",
    "                \n",
    "                self.conv1d1 = nn.Conv1d(16*58*58, 1024, kernel_size=1)\n",
    "                self.conv1d2 = nn.Conv1d(1024,512, kernel_size=1)\n",
    "                self.conv1d3 = nn.Conv1d(512,256, kernel_size=1)\n",
    "\n",
    "                self.upconv1 = nn.ConvTranspose2d(16, 32, kernel_size=3, stride=1)\n",
    "                self.upconv2 = nn.ConvTranspose2d(32, 64, kernel_size=3, stride=1)\n",
    "                self.upconv3 = nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1)\n",
    "                \n",
    "   \n",
    "        def forward(self,x):\n",
    "                #Encodings\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x = F.relu(self.conv3(x))\n",
    "                #print(\"X shape\", x.shape) # torch.Size([1, 16, 58, 58])\n",
    "                \n",
    "                #Style vector\n",
    "                x_style = torch.flatten(x,1)\n",
    "                x_style = x_style.unsqueeze(-1)\n",
    "                x_style = F.relu(self.conv1d1(x_style))\n",
    "                x_style = F.relu(self.conv1d2(x_style))\n",
    "                x_style = F.relu(self.conv1d3(x_style))\n",
    "                #print(\"Final X_Style: \", x_style.shape)\n",
    "\n",
    "                #Image \n",
    "                x_image = F.relu(self.upconv1(x))\n",
    "                x_image = F.relu(self.upconv2(x_image))\n",
    "                x_image = self.upconv3(x_image)\n",
    "            \n",
    "#                 x_style = inverse_normalization(x_style)\n",
    "#                 x_image = inverse_normalization(x_image)\n",
    "                \n",
    "#                 x_image = inverse_normalization(x_image)\n",
    "                \n",
    "                return x_style, x_image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "        def __init__(self):\n",
    "              super(decoder, self).__init__()\n",
    "                  \n",
    "              #Convert style for concat\n",
    "              self.conv1d0 = nn.Conv1d(256,512, kernel_size=1)\n",
    "\n",
    "              #Convert image for concat\n",
    "              self.conv1 = nn.Conv2d(1,64, kernel_size=3)\n",
    "              self.conv2 = nn.Conv2d(64,32, kernel_size=3)\n",
    "              self.conv3 = nn.Conv2d(32,16, kernel_size=3)\n",
    "              self.conv1d1 = nn.Conv1d(16*58*58, 1024, kernel_size=1)\n",
    "              self.conv1d2 = nn.Conv1d(1024,512, kernel_size=1)\n",
    "\n",
    "              #Convert image\n",
    "              self.conv1d3 = nn.Conv1d(1024, 1024, kernel_size=1)\n",
    "              self.conv1d4 = nn.Conv1d(1024, 1024, kernel_size=1)\n",
    "\n",
    "              #Using Upsample + Upconv\n",
    "   \n",
    "              self.upsample1 = nn.Upsample(scale_factor=2,mode='bilinear')\n",
    "              self.upconv1 = nn.ConvTranspose2d(16, 16, 1, stride=1)\n",
    "              self.upsample2 = nn.Upsample(scale_factor=2,mode='bilinear')\n",
    "              self.upconv2 = nn.ConvTranspose2d(16, 16, 1, stride=1)              \n",
    "              self.upsample3 = nn.Upsample(scale_factor=2,mode='bilinear')\n",
    "              self.upconv3 = nn.ConvTranspose2d(16, 3, 1, stride=1)\n",
    "       \n",
    "              \n",
    "                \n",
    "\n",
    "        def forward(self,x_style,x_image):\n",
    "              \n",
    "#               x_image /= 255.0 \n",
    "              #x_style = torch.from_numpy(x_style)\n",
    "              #x_image = torch.from_numpy(x_image)\n",
    "                \n",
    "              x1 = F.relu(self.conv1d0(x_style)) # 512 style\n",
    "            \n",
    "              x2 = F.relu(self.conv1(x_image))\n",
    "              x2 = F.relu(self.conv2(x2))\n",
    "              x2 = F.relu(self.conv3(x2))\n",
    "              x2 = torch.flatten(x2, 1)\n",
    "              x2 = x2.unsqueeze(-1)\n",
    "              x2 = F.relu(self.conv1d1(x2))\n",
    "              x2 = F.relu(self.conv1d2(x2))     # 512 image\n",
    "\n",
    "              x = torch.cat([x1,x2], dim=1) #Linear concatenate, output = [1,1024]\n",
    "\n",
    "              x = F.relu(self.conv1d3(x))\n",
    "              x = F.relu(self.conv1d4(x))\n",
    "              x = (torch.reshape(x, (16,8,8))).unsqueeze(dim=0)\n",
    "\n",
    "              x = self.upsample1(x)\n",
    "              x = F.relu(self.upconv1(x))\n",
    "              x = self.upsample2(x)\n",
    "              x = F.relu(self.upconv2(x))\n",
    "              x = self.upsample3(x)\n",
    "              x = self.upconv3(x)\n",
    "\n",
    "#               x = inverse_normalization(x)\n",
    "            \n",
    "              return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "      super(MyModel, self).__init__()\n",
    "      self.enc = enc\n",
    "      self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "      style, output_mask = self.enc(x)\n",
    "      output_img = self.dec(style, output_mask)\n",
    "      return output_img, output_mask, style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder()\n",
    "dec = decoder()\n",
    "model = MyModel(enc,dec)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = ssim_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hp1/.local/lib/python3.8/site-packages/torch/nn/functional.py:3451: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/home/hp1/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1, 3, 64, 64])) that is different to the input size (torch.Size([3, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/hp1/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1, 1, 64, 64])) that is different to the input size (torch.Size([1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gt_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9bbc0b5adeaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mloss_out_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_base_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mloss_ss_out_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss_out_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_ss_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mloss_sm_out_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm_out_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Same mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_out_mask\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_ss_out_mask\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_sm_out_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gt_mask' is not defined"
     ]
    }
   ],
   "source": [
    "styles = os.listdir('./data/styles1/') # Path to folder containing all styles\n",
    "styles.append(styles[0]) \n",
    "\n",
    "masks = os.listdir('./data/masks1/') # Path to Masks\n",
    "masks.sort()\n",
    "masks.append(masks[0])\n",
    "\n",
    "styles_loc = './data/styles1/' #Style names\n",
    "masks_loc = './data/masks1/'\n",
    "\n",
    "num_styles = 5\n",
    "num_ips = 3 # ips = images per style\n",
    "num_epochs = 10\n",
    "\n",
    "epoch_loss = []\n",
    "dataset_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    dataset_loss = 0\n",
    "    \n",
    "    for i in range(num_styles): # Number of styles \n",
    "    \n",
    "        s1_loc = styles_loc + styles[i] + '/' # Style 1 images\n",
    "        s2_loc = styles_loc + styles[i+1] + '/' # Style 2 images\n",
    "    \n",
    "        s1_images = os.listdir(s1_loc)\n",
    "        s1_images.sort()\n",
    "        s1_images.append(s1_images[0])\n",
    "    \n",
    "        s2_images = os.listdir(s2_loc)\n",
    "        s2_images.sort()\n",
    "        s2_images.append(s2_images[0])\n",
    "    \n",
    "        for j in range(num_ips): # Number of images per style\n",
    "            base_image_loc = s1_loc + s1_images[j]\n",
    "            base_mask_loc = masks_loc + masks[j]       \n",
    "            samestyle_image_loc = s1_loc + s1_images[j+1]\n",
    "            samestyle_mask_loc = masks_loc + masks[j+1]        \n",
    "            samemask_image_loc = s2_loc + s2_images[j]\n",
    "            samemask_mask_loc = masks_loc + masks[j]\n",
    "        \n",
    "            #print(\"Base_image: \", base_image)\n",
    "            #print(\"Base_mask: \", base_mask_loc)\n",
    "            #print(\"samestyle_image: \", samestyle_image)\n",
    "            #print(\"samestyle_mask: \", samestyle_mask)\n",
    "            #print(\"samemask_image: \", samemask_image)\n",
    "            #print(\"samemask_mask: \", samemask_mask)\n",
    "            #print(\"-----\"*10) \n",
    "        \n",
    "            gt_base_image = get_imagetensor(base_image_loc)\n",
    "            gt_base_mask = get_imagetensor(base_mask_loc, mask=1)\n",
    "            gt_ss_image = get_imagetensor(samestyle_image_loc)\n",
    "            gt_ss_mask = get_imagetensor(samestyle_mask_loc, mask=1)        \n",
    "            gt_sm_image = get_imagetensor(samemask_image_loc)\n",
    "            gt_sm_mask = get_imagetensor(samemask_mask_loc, mask=1)    \n",
    "            \n",
    "            # -------------------Training --------------------------------------\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "    \n",
    "            # Full pass\n",
    "            out_image, out_mask, out_style = model(gt_base_image)\n",
    "            ss_out_image, ss_out_mask, ss_out_style = model(gt_ss_image)\n",
    "            sm_out_image, sm_out_mask, sm_out_style = model(gt_sm_image)\n",
    "    \n",
    "            # Reconstruction/Style loss\n",
    "            loss_out_style = criterion(out_image.squeeze(0), gt_base_image)\n",
    "            loss_ss_out_style = criterion(ss_out_image.squeeze(0), gt_ss_image)    \n",
    "            loss_sm_out_style = criterion(sm_out_image.squeeze(0), gt_sm_image)\n",
    "            loss_style = loss_out_style + loss_ss_out_style + loss_sm_out_style\n",
    "    \n",
    "            # Mask Loss\n",
    "            loss_out_mask = criterion(out_mask.squeeze(0), gt_base_mask)\n",
    "            loss_ss_out_mask = criterion(ss_out_mask.squeeze(0), gt_ss_mask)    \n",
    "            loss_sm_out_mask = criterion(sm_out_mask.squeeze(0), gt_base_mask) # Same mask\n",
    "            loss_mask = loss_out_mask + loss_ss_out_mask + loss_sm_out_mask\n",
    "    \n",
    "            # First Image style (Blue) + Second Image Mask (Zero) --> Expected Image 2 i.e samestyle image\n",
    "            out12_image = model.dec(out_style, ss_out_mask)\n",
    "            loss_12 = criterion(out12_image.squeeze(0), gt_ss_image)\n",
    "    \n",
    "            # Third Image style (Green) + First Image Mask (H) --> Expected Image 3 i.e samemask image\n",
    "            out13_image = model.dec(sm_out_style, out_mask)\n",
    "            loss_13 = criterion(out13_image.squeeze(0), gt_sm_image)\n",
    "    \n",
    "            # Same style vector loss\n",
    "            loss_samestyle = criterion(out_style, ss_out_style)\n",
    "    \n",
    "            # Same mask loss\n",
    "            loss_samemask = criterion(out_mask, sm_out_mask)\n",
    "    \n",
    "            total_loss = loss_style + loss_mask + loss_12 + loss_13 + loss_samestyle + loss_samemask\n",
    "            \n",
    "            dataset_loss += total_loss\n",
    "            \n",
    "            #display_losses(epoch, total_loss, loss_mask, loss_style, loss_12, loss_13)\n",
    "            #loss.append(total_loss.item())\n",
    "            print(\"-----\"*20)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    epoch_loss.append(dataset_loss) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled17.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vocc",
   "language": "python",
   "name": "vocc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
